{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0]\n",
      "[datetime.datetime(2023, 6, 10, 0, 0), datetime.datetime(2023, 6, 10, 0, 0), datetime.datetime(2023, 6, 10, 0, 0), datetime.datetime(2023, 6, 10, 0, 0)]\n",
      "[20230610000000, 20230610000000, 20230610000000, 20230610000000]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "schema = [{'name': 'id', \"type\":\"id\" }, {'name':'feature1', 'type':'float'}, {'name':'feature2', 'type':'string'},{'name':'feature3', 'type':'datetime'},{'name':'feature4', 'type':'bool'}]\n",
    "features = [[1,2,3,4], [10.1, 11.1, 12.1, 13.1], ['hello', 'hi','bye','dd'],[datetime(2023,6,10),datetime(2023,6,10),datetime(2023,6,10),datetime(2023,6,10)],[True, False, True, False]]\n",
    "tags = [[1, 0], [2, 0],[3,1], [4, 1]]\n",
    "date = [datetime(2023,6,10),datetime(2023,6,10),datetime(2023,6,10),datetime(2023,6,10)]\n",
    "bo = [True, False, True, False]\n",
    "print([int(x) for x in bo])\n",
    "print(date)\n",
    "print([int(x.strftime(\"%Y%m%d%H%M%S\")) for x in date])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique ['hi', 'hello', 'dd', 'bye']\n",
      "mapping {'hi': 0, 'hello': 1, 'dd': 2, 'bye': 3}\n",
      "[[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n",
      "[-1.627361387260298, -1.1717001988274145, -0.2603778219616476, 0.1952833664712359, 0.6509445549041194, 1.106605743337003, 1.106605743337003]\n",
      "[0.0, 0.16666666666666666, 0.5, 0.6666666666666666, 0.8333333333333334, 1.0, 1.0]\n",
      "unique ['hi', 'hello', 'dd', 'bye']\n",
      "mapping {'hi': 0, 'hello': 1, 'dd': 2, 'bye': 3}\n",
      "scaleX is:  [[0.0, 0.3333333333333333, 0.6666666666666666, 1.0], [0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0], [20230610000000, 20230610000000, 20230610000000, 20230610000000], [1, 0, 1, 0]]\n",
      "([[0.0, 0, 1, 0, 0, 20230610000000, 1], [0.3333333333333333, 1, 0, 0, 0, 20230610000000, 0], [0.6666666666666666, 0, 0, 0, 1, 20230610000000, 1], [1.0, 0, 0, 1, 0, 20230610000000, 0]], [0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def one_hot_encode(arr):\n",
    "    unique = list(set(arr))\n",
    "    print(\"unique\", unique)\n",
    "    mapping = {}\n",
    "    for i, x in enumerate(unique):\n",
    "        mapping[x] = i\n",
    "    print(\"mapping\", mapping)\n",
    "    one_hot_encoder = []\n",
    "    for a in arr:\n",
    "        res = [0]*len(arr)\n",
    "        res[mapping[a]] = 1\n",
    "        one_hot_encoder.append(res)\n",
    "    return one_hot_encoder\n",
    "print(one_hot_encode(['hello', 'hi','bye','dd']))\n",
    "\n",
    "def get_std_scaler(arr):\n",
    "    arr_mean = sum(arr)/len(arr)\n",
    "    var = sum(pow(x-arr_mean,2) for x in arr)/len(arr)\n",
    "    std = math.sqrt(var)\n",
    "    return [(x-arr_mean)/std for x in arr]\n",
    "\n",
    "def get_minmax_scaler(arr):\n",
    "    arr_min, arr_max = min(arr), max(arr)\n",
    "    sub = arr_max-arr_min\n",
    "    return [(x-arr_min)/sub for x in arr]\n",
    "\n",
    "arr = [1,2,4,5,6,7,7]\n",
    "print(get_std_scaler(arr))\n",
    "print(get_minmax_scaler(arr))\n",
    "\n",
    "def preprossing(schema, features, tags):\n",
    "    scaledX = []\n",
    "    lables = {}\n",
    "    for id, l in tags:\n",
    "        lables[id] = l\n",
    "    y = [lables[x] for x in features[0]]\n",
    "    for i in range(1, len(schema)):\n",
    "        if schema[i]['type'] == 'float':\n",
    "            scaledX.append(get_minmax_scaler(features[i]))\n",
    "        elif schema[i]['type'] == 'string':\n",
    "            encoders = one_hot_encode(features[i])\n",
    "            for e in encoders:\n",
    "                scaledX.append(e)\n",
    "        elif schema[i]['type'] == 'datetime':\n",
    "            scaledX.append([int(x.strftime(\"%Y%m%d%H%M%S\")) for x in features[i]])\n",
    "        elif schema[i]['type'] == 'bool':\n",
    "            scaledX.append([int(x) for x in features[i]])\n",
    "    print(\"scaleX is: \", scaledX)\n",
    "    \n",
    "    \n",
    "    return list(map(list, zip(*scaledX))), y\n",
    "print(preprossing(schema, features, tags))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Employee data : \n",
      "   Employee id  Gender_F  Gender_M  Remarks_Good  Remarks_Great  Remarks_Nice\n",
      "0           10       0.0       1.0           1.0            0.0           0.0\n",
      "1           20       1.0       0.0           0.0            0.0           1.0\n",
      "2           15       1.0       0.0           1.0            0.0           0.0\n",
      "3           25       0.0       1.0           0.0            1.0           0.0\n",
      "4           30       1.0       0.0           0.0            0.0           1.0\n"
     ]
    }
   ],
   "source": [
    "#%pip install scikit-learn\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "data = {'Employee id': [10, 20, 15, 25, 30],\n",
    "        'Gender': ['M', 'F', 'F', 'M', 'F'],\n",
    "        'Remarks': ['Good', 'Nice', 'Good', 'Great', 'Nice'],\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "one_hot_encoded = encoder.fit_transform(df[categorical_columns])\n",
    "\n",
    "one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "df_encoded = pd.concat([df, one_hot_df], axis=1)\n",
    "\n",
    "df_encoded = df_encoded.drop(categorical_columns, axis=1)\n",
    "\n",
    "print(f\"Encoded Employee data : \\n{df_encoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'red': 0, 'green': 1, 'blue': 2, 'black': 3, 'yellow': 4}\n",
      "[[1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 0, 0, 1], [1, 0, 0, 0, 0], [0, 0, 1, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "#implement one hot encoding without calling sklearn\n",
    "import numpy as np\n",
    "\n",
    "colors = [\"red\", \"green\", \"yellow\", \"red\", \"blue\"]\n",
    "total_colors = [\"red\", \"green\", \"blue\", \"black\", \"yellow\"]\n",
    "mapping = {}\n",
    "for x in range(len(total_colors)):\n",
    "    mapping[total_colors[x]] = x\n",
    "one_hot_encoder= []\n",
    "#print(mapping)\n",
    "for c in colors:\n",
    "    arr = list(np.zeros(len(total_colors), dtype=int))\n",
    "    arr[mapping[c]] = 1\n",
    "    one_hot_encoder.append(arr)\n",
    "print(one_hot_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 10, 0.5, 1, 0, 18993, 1], [2, 20, 1.5, 0, 1, 19358, 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import datetime\n",
    "\n",
    "def one_hot_encode(strings):\n",
    "    # Identify all unique string values\n",
    "    unique_strings = list(set(strings))\n",
    "    unique_strings.sort()  # Sorting to ensure consistent ordering\n",
    "\n",
    "    # Create a dictionary to map each unique string to an index\n",
    "    string_to_index = {string: index for index, string in enumerate(unique_strings)}\n",
    "\n",
    "    # Create the one-hot encoded vectors\n",
    "    one_hot_vectors = []\n",
    "    for string in strings:\n",
    "        vector = [0] * len(unique_strings)\n",
    "        index = string_to_index[string]\n",
    "        vector[index] = 1\n",
    "        one_hot_vectors.append(vector)\n",
    "\n",
    "    return one_hot_vectors\n",
    "\n",
    "def prepare_data_for_logistic_regression(schema, training_data, label_data):\n",
    "    # Helper functions for type conversion\n",
    "    def convert_value(value, type_name):\n",
    "        if type_name == 'int':\n",
    "            return int(value)\n",
    "        elif type_name == 'float':\n",
    "            return float(value)\n",
    "        elif type_name == 'bool':\n",
    "            return 1 if value else 0\n",
    "        elif type_name == 'str':\n",
    "            return value  # We'll handle string conversion separately\n",
    "        elif type_name == 'date':\n",
    "            return (datetime.datetime.strptime(value, \"%Y-%m-%d\") - datetime.datetime(1970, 1, 1)).days\n",
    "        elif type_name == 'datetime':\n",
    "            return (datetime.datetime.strptime(value, \"%Y-%m-%d %H:%M:%S\") - datetime.datetime(1970, 1, 1)).total_seconds()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported type: {type_name}\")\n",
    "\n",
    "    # Create a list of column names and types\n",
    "    column_names = [col['name'] for col in schema]\n",
    "    column_types = {col['name']: col['type'] for col in schema}\n",
    "\n",
    "    # Collect all string columns for one-hot encoding\n",
    "    string_columns = [col['name'] for col in schema if col['type'] == 'str']\n",
    "    string_values = {col: [] for col in string_columns}\n",
    "\n",
    "    # Gather string values for each string column\n",
    "    for record in training_data:\n",
    "        for col in string_columns:\n",
    "            idx = column_names.index(col)\n",
    "            string_values[col].append(record[idx])\n",
    "\n",
    "    # Perform one-hot encoding for all string columns\n",
    "    one_hot_encoded = {col: one_hot_encode(string_values[col]) for col in string_columns}\n",
    "\n",
    "    # Prepare the feature vectors\n",
    "    feature_vectors = []\n",
    "    for record in training_data:\n",
    "        feature_vector = []\n",
    "        for i, value in enumerate(record):\n",
    "            column_name = column_names[i]\n",
    "            column_type = column_types[column_name]\n",
    "            if column_type != 'str':\n",
    "                feature_vector.append(convert_value(value, column_type))\n",
    "            else:\n",
    "                # Append one-hot encoded vector for this string value\n",
    "                feature_vector.extend(one_hot_encoded[column_name].pop(0))\n",
    "        \n",
    "        # Append the label at the end of the feature vector\n",
    "        record_id = record[0]\n",
    "        label = label_data.get(record_id, 0)\n",
    "        feature_vector.append(label)\n",
    "        feature_vectors.append(feature_vector)\n",
    "\n",
    "    return feature_vectors\n",
    "\n",
    "# Example usage:\n",
    "schema = [\n",
    "    {'type': 'int', 'name': 'id'},\n",
    "    {'type': 'int', 'name': 'feature1'},\n",
    "    {'type': 'float', 'name': 'feature2'},\n",
    "    {'type': 'str', 'name': 'feature3'},\n",
    "    {'type': 'date', 'name': 'feature4'}\n",
    "]\n",
    "\n",
    "training_data = [\n",
    "    [1, 10, 0.5, 'A', '2022-01-01'],\n",
    "    [2, 20, 1.5, 'B', '2023-01-01']\n",
    "]\n",
    "\n",
    "label_data = {\n",
    "    1: 1,\n",
    "    2: 0\n",
    "}\n",
    "\n",
    "feature_vectors = prepare_data_for_logistic_regression(schema, training_data, label_data)\n",
    "print(feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'a': ['b', 'c'], 'b': ['c'], 'c': ['b', 'd'], 'd': ['c'], 'e': ['d', 'c']})\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from collections import defaultdict\n",
    "def solution(words):\n",
    "    \n",
    "    counters = {x: defaultdict(int) for x in list(string.ascii_lowercase)}\n",
    "    maximum = {x: 0 for x in list(string.ascii_lowercase)}\n",
    "    \n",
    "    for word in words:\n",
    "        chars = set(word)\n",
    "        for c1 in chars:\n",
    "            for c2 in chars:\n",
    "                if c1 != c2:\n",
    "                    counters[c1][c2]+=1\n",
    "                    maximum[c1] = max(maximum[c1], counters[c1][c2])\n",
    "    res = defaultdict(list)\n",
    "    for c in maximum.keys():\n",
    "        for letter, count in counters[c].items():\n",
    "            if count == maximum[c]:\n",
    "                res[c].append(letter)\n",
    "    return res\n",
    "\n",
    "print(solution(['abc', 'bcd', 'cde']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
